# Move logic to remove kafka filter to output to handle data larger than 1500
filter {
  mutate {
    # The Kafka and offset fields are internals that are not required
    # in ElasticSearch
    remove_field => [ "kafka", "offset" ]
  }
}

filter {
  if "REPLACE_ESALIAS_ENABLE_DATE" != "" {
      ruby {
            code => "
                # If you need to debug this, uncomment the two lines below
                # event.to_hash['enable_date'] = Date.parse('REPLACE_ESALIAS_ENABLE_DATE').to_s
                # event.to_hash['log_date'] = Date.parse(event['@timestamp'].to_iso8601).to_s
                if (Date.parse('REPLACE_ESALIAS_ENABLE_DATE') > Date.parse(event['@timestamp'].to_iso8601))
                    event.tag('logmet_before_alias_date')
                end
            "
      }
  }
}

#Output file for ops visibility project
output {
    if ("ESALIAS_FUNCTION_ENABLE" == "true") and ("logmet_before_alias_date" not in [tags]) {
      elasticsearch {
        hosts => [ "REPLACE_ELASTICSEARCH_HTTP_NODE_LIST" ]
        retry_max_interval => 60
        max_retries => 100
        index => "logstash-%{+YYYY.MM.dd}"
        workers => 4
        flush_size => REPLACE_FLUSH_SIZE
        document_id => "%{event_uuid}" 
      }
    } else {
      elasticsearch {
        hosts => [ "REPLACE_ELASTICSEARCH_HTTP_NODE_LIST" ]
        retry_max_interval => 60
        max_retries => 100
        index => "logstash-%{ALCH_TENANT_ID}-%{+YYYY.MM.dd}"
        workers => 4
        flush_size => REPLACE_FLUSH_SIZE
        document_id => "%{event_uuid}"
      }
    }
}

